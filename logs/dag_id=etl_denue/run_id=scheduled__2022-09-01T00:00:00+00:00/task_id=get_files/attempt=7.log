[2022-10-30 05:36:09,775] {taskinstance.py:1179} INFO - Dependencies all met for <TaskInstance: etl_denue.get_files scheduled__2022-09-01T00:00:00+00:00 [queued]>
[2022-10-30 05:36:09,781] {taskinstance.py:1179} INFO - Dependencies all met for <TaskInstance: etl_denue.get_files scheduled__2022-09-01T00:00:00+00:00 [queued]>
[2022-10-30 05:36:09,782] {taskinstance.py:1376} INFO - 
--------------------------------------------------------------------------------
[2022-10-30 05:36:09,782] {taskinstance.py:1377} INFO - Starting attempt 7 of 12
[2022-10-30 05:36:09,782] {taskinstance.py:1378} INFO - 
--------------------------------------------------------------------------------
[2022-10-30 05:36:09,790] {taskinstance.py:1397} INFO - Executing <Task(SeleniumOperator): get_files> on 2022-09-01 00:00:00+00:00
[2022-10-30 05:36:09,795] {standard_task_runner.py:52} INFO - Started process 1814 to run task
[2022-10-30 05:36:09,797] {standard_task_runner.py:79} INFO - Running: ['***', 'tasks', 'run', 'etl_denue', 'get_files', 'scheduled__2022-09-01T00:00:00+00:00', '--job-id', '9', '--raw', '--subdir', 'DAGS_FOLDER/get_raw_files/scrapper.py', '--cfg-path', '/tmp/tmpvqewglev', '--error-file', '/tmp/tmpkswn8sp3']
[2022-10-30 05:36:09,798] {standard_task_runner.py:80} INFO - Job 9: Subtask get_files
[2022-10-30 05:36:09,831] {task_command.py:371} INFO - Running <TaskInstance: etl_denue.get_files scheduled__2022-09-01T00:00:00+00:00 [running]> on host aa62e71d0153
[2022-10-30 05:36:09,872] {taskinstance.py:1591} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=Freakisimo
AIRFLOW_CTX_DAG_ID=etl_denue
AIRFLOW_CTX_TASK_ID=get_files
AIRFLOW_CTX_EXECUTION_DATE=2022-09-01T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=7
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2022-09-01T00:00:00+00:00
[2022-10-30 05:36:09,967] {selenium_hook.py:14} INFO - initialised hook
[2022-10-30 05:36:09,967] {selenium_hook.py:18} INFO - getting ip
[2022-10-30 05:36:09,967] {selenium_hook.py:31} INFO - creating driver
[2022-10-30 05:36:10,062] {selenium_hook.py:46} INFO - remote ready
[2022-10-30 05:36:10,063] {logging_mixin.py:115} INFO - https://www.inegi.org.mx/app/descarga/
[2022-10-30 05:36:12,128] {logging_mixin.py:115} INFO - TÃ­tulo
Periodo
Formatos
DENUE
[2022-10-30 05:36:12,243] {taskinstance.py:1420} INFO - Marking task as SUCCESS. dag_id=etl_denue, task_id=get_files, execution_date=20220901T000000, start_date=20221030T053609, end_date=20221030T053612
[2022-10-30 05:36:12,293] {local_task_job.py:156} INFO - Task exited with return code 0
[2022-10-30 05:36:12,320] {local_task_job.py:273} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2022-10-30 05:56:03,086] {taskinstance.py:1179} INFO - Dependencies all met for <TaskInstance: etl_denue.get_files scheduled__2022-09-01T00:00:00+00:00 [queued]>
[2022-10-30 05:56:03,091] {taskinstance.py:1179} INFO - Dependencies all met for <TaskInstance: etl_denue.get_files scheduled__2022-09-01T00:00:00+00:00 [queued]>
[2022-10-30 05:56:03,091] {taskinstance.py:1376} INFO - 
--------------------------------------------------------------------------------
[2022-10-30 05:56:03,091] {taskinstance.py:1377} INFO - Starting attempt 7 of 12
[2022-10-30 05:56:03,091] {taskinstance.py:1378} INFO - 
--------------------------------------------------------------------------------
[2022-10-30 05:56:03,099] {taskinstance.py:1397} INFO - Executing <Task(SeleniumOperator): get_files> on 2022-09-01 00:00:00+00:00
[2022-10-30 05:56:03,105] {standard_task_runner.py:52} INFO - Started process 1175 to run task
[2022-10-30 05:56:03,106] {standard_task_runner.py:79} INFO - Running: ['***', 'tasks', 'run', 'etl_denue', 'get_files', 'scheduled__2022-09-01T00:00:00+00:00', '--job-id', '9', '--raw', '--subdir', 'DAGS_FOLDER/get_raw_files/scrapper.py', '--cfg-path', '/tmp/tmpegg63e7a', '--error-file', '/tmp/tmp4me2ost_']
[2022-10-30 05:56:03,107] {standard_task_runner.py:80} INFO - Job 9: Subtask get_files
[2022-10-30 05:56:03,139] {task_command.py:371} INFO - Running <TaskInstance: etl_denue.get_files scheduled__2022-09-01T00:00:00+00:00 [running]> on host f80d6ee97b6d
[2022-10-30 05:56:03,180] {taskinstance.py:1591} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=Freakisimo
AIRFLOW_CTX_DAG_ID=etl_denue
AIRFLOW_CTX_TASK_ID=get_files
AIRFLOW_CTX_EXECUTION_DATE=2022-09-01T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=7
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2022-09-01T00:00:00+00:00
[2022-10-30 05:56:03,274] {selenium_hook.py:14} INFO - initialised hook
[2022-10-30 05:56:03,274] {selenium_hook.py:18} INFO - getting ip
[2022-10-30 05:56:03,274] {selenium_hook.py:31} INFO - creating driver
[2022-10-30 05:56:03,369] {selenium_hook.py:46} INFO - remote ready
[2022-10-30 05:56:05,698] {logging_mixin.py:115} INFO - re.compile('.*en formato csv*.')
[2022-10-30 05:56:05,700] {taskinstance.py:1909} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/opt/airflow/plugins/operators/selenium_operator.py", line 24, in execute
    hook.run_script(self.script, self.script_args)
  File "/opt/airflow/plugins/hooks/selenium_hook.py", line 56, in run_script
    script(self.driver, *args)
  File "/opt/airflow/dags/utils/get_files.py", line 35, in selenium_scrapper
    links = [a['href'] for a in soup.find_all(label)]
  File "/opt/airflow/dags/utils/get_files.py", line 35, in <listcomp>
    links = [a['href'] for a in soup.find_all(label)]
  File "/home/airflow/.local/lib/python3.7/site-packages/bs4/element.py", line 1519, in __getitem__
    return self.attrs[key]
KeyError: 'href'
[2022-10-30 05:56:05,706] {taskinstance.py:1420} INFO - Marking task as UP_FOR_RETRY. dag_id=etl_denue, task_id=get_files, execution_date=20220901T000000, start_date=20221030T055603, end_date=20221030T055605
[2022-10-30 05:56:05,713] {standard_task_runner.py:97} ERROR - Failed to execute job 9 for task get_files ('href'; 1175)
[2022-10-30 05:56:05,723] {local_task_job.py:156} INFO - Task exited with return code 1
[2022-10-30 05:56:05,748] {local_task_job.py:273} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2022-10-30 06:36:03,651] {taskinstance.py:1179} INFO - Dependencies all met for <TaskInstance: etl_denue.get_files scheduled__2022-09-01T00:00:00+00:00 [queued]>
[2022-10-30 06:36:03,656] {taskinstance.py:1179} INFO - Dependencies all met for <TaskInstance: etl_denue.get_files scheduled__2022-09-01T00:00:00+00:00 [queued]>
[2022-10-30 06:36:03,656] {taskinstance.py:1376} INFO - 
--------------------------------------------------------------------------------
[2022-10-30 06:36:03,656] {taskinstance.py:1377} INFO - Starting attempt 7 of 12
[2022-10-30 06:36:03,656] {taskinstance.py:1378} INFO - 
--------------------------------------------------------------------------------
[2022-10-30 06:36:03,664] {taskinstance.py:1397} INFO - Executing <Task(SeleniumOperator): get_files> on 2022-09-01 00:00:00+00:00
[2022-10-30 06:36:03,670] {standard_task_runner.py:52} INFO - Started process 3289 to run task
[2022-10-30 06:36:03,672] {standard_task_runner.py:79} INFO - Running: ['***', 'tasks', 'run', 'etl_denue', 'get_files', 'scheduled__2022-09-01T00:00:00+00:00', '--job-id', '9', '--raw', '--subdir', 'DAGS_FOLDER/get_raw_files/scrapper.py', '--cfg-path', '/tmp/tmp0zk4_ss3', '--error-file', '/tmp/tmp7nmlvb7c']
[2022-10-30 06:36:03,673] {standard_task_runner.py:80} INFO - Job 9: Subtask get_files
[2022-10-30 06:36:03,706] {task_command.py:371} INFO - Running <TaskInstance: etl_denue.get_files scheduled__2022-09-01T00:00:00+00:00 [running]> on host 1dd15365b03f
[2022-10-30 06:36:03,749] {taskinstance.py:1591} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=Freakisimo
AIRFLOW_CTX_DAG_ID=etl_denue
AIRFLOW_CTX_TASK_ID=get_files
AIRFLOW_CTX_EXECUTION_DATE=2022-09-01T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=7
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2022-09-01T00:00:00+00:00
[2022-10-30 06:36:03,848] {selenium_hook.py:14} INFO - initialised hook
[2022-10-30 06:36:03,848] {selenium_hook.py:18} INFO - getting ip
[2022-10-30 06:36:03,848] {selenium_hook.py:31} INFO - creating driver
[2022-10-30 06:36:03,948] {selenium_hook.py:46} INFO - remote ready
[2022-10-30 06:36:05,955] {logging_mixin.py:115} INFO - 'WebDriver' object has no attribute 'find_element_by_id'
[2022-10-30 06:36:06,025] {taskinstance.py:1909} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/opt/airflow/plugins/operators/selenium_operator.py", line 24, in execute
    hook.run_script(self.script, self.script_args)
  File "/opt/airflow/plugins/hooks/selenium_hook.py", line 56, in run_script
    script(self.driver, *args)
  File "/opt/airflow/dags/utils/get_files.py", line 35, in selenium_scrapper
    soup = BeautifulSoup(innerHTML, 'html.parser')
UnboundLocalError: local variable 'innerHTML' referenced before assignment
[2022-10-30 06:36:06,031] {taskinstance.py:1420} INFO - Marking task as UP_FOR_RETRY. dag_id=etl_denue, task_id=get_files, execution_date=20220901T000000, start_date=20221030T063603, end_date=20221030T063606
[2022-10-30 06:36:06,038] {standard_task_runner.py:97} ERROR - Failed to execute job 9 for task get_files (local variable 'innerHTML' referenced before assignment; 3289)
[2022-10-30 06:36:06,047] {local_task_job.py:156} INFO - Task exited with return code 1
[2022-10-30 06:36:06,071] {local_task_job.py:273} INFO - 0 downstream tasks scheduled from follow-on schedule check
