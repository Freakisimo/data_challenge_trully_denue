[2022-10-30 07:35:42,201] {taskinstance.py:1179} INFO - Dependencies all met for <TaskInstance: etl_denue.get_files scheduled__2022-09-01T00:00:00+00:00 [queued]>
[2022-10-30 07:35:42,207] {taskinstance.py:1179} INFO - Dependencies all met for <TaskInstance: etl_denue.get_files scheduled__2022-09-01T00:00:00+00:00 [queued]>
[2022-10-30 07:35:42,207] {taskinstance.py:1376} INFO - 
--------------------------------------------------------------------------------
[2022-10-30 07:35:42,207] {taskinstance.py:1377} INFO - Starting attempt 17 of 22
[2022-10-30 07:35:42,207] {taskinstance.py:1378} INFO - 
--------------------------------------------------------------------------------
[2022-10-30 07:35:42,214] {taskinstance.py:1397} INFO - Executing <Task(SeleniumOperator): get_files> on 2022-09-01 00:00:00+00:00
[2022-10-30 07:35:42,221] {standard_task_runner.py:52} INFO - Started process 8369 to run task
[2022-10-30 07:35:42,222] {standard_task_runner.py:79} INFO - Running: ['***', 'tasks', 'run', 'etl_denue', 'get_files', 'scheduled__2022-09-01T00:00:00+00:00', '--job-id', '19', '--raw', '--subdir', 'DAGS_FOLDER/get_raw_files/scrapper.py', '--cfg-path', '/tmp/tmpqu0h32oj', '--error-file', '/tmp/tmpxx5l5i5n']
[2022-10-30 07:35:42,223] {standard_task_runner.py:80} INFO - Job 19: Subtask get_files
[2022-10-30 07:35:42,257] {task_command.py:371} INFO - Running <TaskInstance: etl_denue.get_files scheduled__2022-09-01T00:00:00+00:00 [running]> on host 1dd15365b03f
[2022-10-30 07:35:42,301] {taskinstance.py:1591} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=Freakisimo
AIRFLOW_CTX_DAG_ID=etl_denue
AIRFLOW_CTX_TASK_ID=get_files
AIRFLOW_CTX_EXECUTION_DATE=2022-09-01T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=17
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2022-09-01T00:00:00+00:00
[2022-10-30 07:35:42,398] {selenium_hook.py:14} INFO - initialised hook
[2022-10-30 07:35:42,398] {selenium_hook.py:18} INFO - getting ip
[2022-10-30 07:35:42,398] {selenium_hook.py:31} INFO - creating driver
[2022-10-30 07:35:42,497] {selenium_hook.py:46} INFO - remote ready
[2022-10-30 07:35:53,058] {logging_mixin.py:115} INFO - Message: 
Stacktrace:
#0 0x561f29ccc2c3 <unknown>
#1 0x561f29ad583a <unknown>
#2 0x561f29b0e985 <unknown>
#3 0x561f29b0eb61 <unknown>
#4 0x561f29b46d14 <unknown>
#5 0x561f29b2cf6d <unknown>
#6 0x561f29b44a50 <unknown>
#7 0x561f29b2cd63 <unknown>
#8 0x561f29b017e3 <unknown>
#9 0x561f29b02a21 <unknown>
#10 0x561f29d1a18e <unknown>
#11 0x561f29d1d622 <unknown>
#12 0x561f29d00aae <unknown>
#13 0x561f29d1e2a3 <unknown>
#14 0x561f29cf4ecf <unknown>
#15 0x561f29d3e588 <unknown>
#16 0x561f29d3e706 <unknown>
#17 0x561f29d588b2 <unknown>
#18 0x7f675f898609 <unknown>
[2022-10-30 07:35:53,135] {taskinstance.py:1909} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/opt/airflow/plugins/operators/selenium_operator.py", line 24, in execute
    hook.run_script(self.script, self.script_args)
  File "/opt/airflow/plugins/hooks/selenium_hook.py", line 56, in run_script
    script(self.driver, *args)
  File "/opt/airflow/dags/utils/get_files.py", line 36, in selenium_scrapper
    soup = BeautifulSoup(innerHTML, 'html.parser')
UnboundLocalError: local variable 'innerHTML' referenced before assignment
[2022-10-30 07:35:53,140] {taskinstance.py:1420} INFO - Marking task as UP_FOR_RETRY. dag_id=etl_denue, task_id=get_files, execution_date=20220901T000000, start_date=20221030T073542, end_date=20221030T073553
[2022-10-30 07:35:53,147] {standard_task_runner.py:97} ERROR - Failed to execute job 19 for task get_files (local variable 'innerHTML' referenced before assignment; 8369)
[2022-10-30 07:35:53,192] {local_task_job.py:156} INFO - Task exited with return code 1
[2022-10-30 07:35:53,215] {local_task_job.py:273} INFO - 0 downstream tasks scheduled from follow-on schedule check
